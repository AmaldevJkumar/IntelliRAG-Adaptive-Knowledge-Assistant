apiVersion: v1
kind: ConfigMap
metadata:
  name: rag-config
  namespace: rag-system
data:
  APP_NAME: "RAG Knowledge Assistant"
  DEBUG: "false"
  HOST: "0.0.0.0"
  PORT: "8000"
  API_PREFIX: "/api/v1"
  
  # Vector Database
  VECTOR_DB_TYPE: "pinecone"
  PINECONE_ENVIRONMENT: "us-west1-gcp-free"
  PINECONE_INDEX_NAME: "rag-knowledge-base"
  PINECONE_DIMENSION: "384"
  
  # LLM Configuration
  LLM_PROVIDER: "openai"
  OPENAI_MODEL: "gpt-4"
  OPENAI_TEMPERATURE: "0.1"
  OPENAI_MAX_TOKENS: "1000"
  
  # Embedding Configuration
  EMBEDDING_MODEL: "sentence-transformers/all-MiniLM-L6-v2"
  EMBEDDING_BATCH_SIZE: "32"
  
  # Document Processing
  CHUNK_SIZE: "1000"
  CHUNK_OVERLAP: "200"
  MAX_FILE_SIZE: "52428800"
  
  # Monitoring
  ENABLE_METRICS: "true"
  ENABLE_MLFLOW: "true"
  LOG_LEVEL: "INFO"
  
  # Performance
  MAX_WORKERS: "4"
  REQUEST_TIMEOUT: "30"
  RATE_LIMIT_REQUESTS: "100"
  RATE_LIMIT_WINDOW: "60"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
  namespace: rag-system
data:
  nginx.conf: |
    events {
        worker_connections 1024;
    }
    
    http {
        upstream rag-backend {
            server rag-backend-service:8000;
        }
        
        server {
            listen 80;
            server_name _;
            
            client_max_body_size 50M;
            
            location / {
                proxy_pass http://rag-backend;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
                proxy_timeout 60s;
            }
            
            location /health {
                proxy_pass http://rag-backend/health;
                access_log off;
            }
        }
    }
