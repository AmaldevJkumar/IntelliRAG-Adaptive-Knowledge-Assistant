{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Knowledge Assistant - Comprehensive Evaluation Notebook\n",
    "\n",
    "This notebook provides comprehensive evaluation and analysis of the RAG Knowledge Assistant system, including performance metrics, quality assessment, and optimization recommendations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Data Loading and Preparation](#data-prep)\n",
    "3. [System Performance Analysis](#performance)\n",
    "4. [Quality Evaluation with RAGAS](#quality)\n",
    "5. [Retrieval Analysis](#retrieval)\n",
    "6. [Generation Assessment](#generation)\n",
    "7. [Comparative Analysis](#comparison)\n",
    "8. [Optimization Recommendations](#optimization)\n",
    "9. [Reporting and Visualization](#reporting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Evaluation libraries\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"ðŸ“Š RAG Evaluation Framework Initialized\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'base_url': 'http://localhost:8000',\n",
    "    'api_prefix': '/api/v1',\n",
    "    'timeout': 30,\n",
    "    'max_retries': 3,\n",
    "    'evaluation_queries': 50,  # Number of queries for evaluation\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(config['random_seed'])\n",
    "\n",
    "print(f\"ðŸ”§ Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation {#data-prep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation queries and expected answers\n",
    "evaluation_queries = [\n",
    "    {\n",
    "        \"query\": \"What is machine learning and how does it work?\",\n",
    "        \"expected_topics\": [\"machine learning\", \"algorithms\", \"data\", \"training\"],\n",
    "        \"difficulty\": \"basic\",\n",
    "        \"category\": \"ml_fundamentals\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain the difference between supervised and unsupervised learning\",\n",
    "        \"expected_topics\": [\"supervised learning\", \"unsupervised learning\", \"labeled data\"],\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"category\": \"ml_fundamentals\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the key components of a RAG system architecture?\",\n",
    "        \"expected_topics\": [\"retrieval\", \"generation\", \"vector database\", \"embedding\"],\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"category\": \"rag_systems\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do you implement enterprise AI governance and compliance?\",\n",
    "        \"expected_topics\": [\"governance\", \"compliance\", \"model management\", \"audit\"],\n",
    "        \"difficulty\": \"advanced\",\n",
    "        \"category\": \"enterprise_ai\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the best practices for vector database optimization?\",\n",
    "        \"expected_topics\": [\"vector database\", \"optimization\", \"indexing\", \"performance\"],\n",
    "        \"difficulty\": \"advanced\",\n",
    "        \"category\": \"rag_systems\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Extend with more queries for comprehensive evaluation\n",
    "additional_queries = [\n",
    "    {\"query\": \"What is the bias-variance tradeoff in machine learning?\", \"difficulty\": \"intermediate\", \"category\": \"ml_fundamentals\"},\n",
    "    {\"query\": \"How do you prevent overfitting in neural networks?\", \"difficulty\": \"intermediate\", \"category\": \"ml_fundamentals\"},\n",
    "    {\"query\": \"Explain the concept of embeddings in RAG systems\", \"difficulty\": \"intermediate\", \"category\": \"rag_systems\"},\n",
    "    {\"query\": \"What are the security considerations for enterprise AI deployment?\", \"difficulty\": \"advanced\", \"category\": \"enterprise_ai\"},\n",
    "    {\"query\": \"How do you evaluate the performance of a RAG system?\", \"difficulty\": \"advanced\", \"category\": \"rag_systems\"}\n",
    "]\n",
    "\n",
    "evaluation_queries.extend(additional_queries)\n",
    "\n",
    "print(f\"ðŸ“‹ Loaded {len(evaluation_queries)} evaluation queries\")\n",
    "print(f\"Categories: {set(q['category'] for q in evaluation_queries)}\")\n",
    "print(f\"Difficulty levels: {set(q['difficulty'] for q in evaluation_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Client for RAG system\n",
    "class RAGEvaluationClient:\n",
    "    def __init__(self, base_url, api_prefix, timeout=30):\n",
    "        self.base_url = base_url\n",
    "        self.api_prefix = api_prefix\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def query(self, text, **kwargs):\n",
    "        \"\"\"Send query to RAG system\"\"\"\n",
    "        url = f\"{self.base_url}{self.api_prefix}/query\"\n",
    "        payload = {\n",
    "            \"query\": text,\n",
    "            \"top_k\": kwargs.get('top_k', 5),\n",
    "            \"query_type\": kwargs.get('query_type', 'hybrid'),\n",
    "            \"include_sources\": True,\n",
    "            \"confidence_threshold\": kwargs.get('confidence_threshold', 0.7)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Query failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def health_check(self):\n",
    "        \"\"\"Check if RAG system is healthy\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/health\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Get system metrics\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}{self.api_prefix}/monitoring/metrics\", timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to get metrics: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize client\n",
    "client = RAGEvaluationClient(\n",
    "    base_url=config['base_url'],\n",
    "    api_prefix=config['api_prefix'],\n",
    "    timeout=config['timeout']\n",
    ")\n",
    "\n",
    "# Health check\n",
    "if client.health_check():\n",
    "    print(\"âœ… RAG system is healthy and ready for evaluation\")\n",
    "else:\n",
    "    print(\"âŒ RAG system is not responding. Please check if it's running.\")\n",
    "    print(f\"Expected URL: {config['base_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Performance Analysis {#performance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "def run_performance_benchmark(client, queries, num_runs=3):\n",
    "    \"\"\"Run performance benchmark with multiple runs\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, query_data in enumerate(queries[:10]):  # First 10 queries for performance test\n",
    "        query = query_data['query']\n",
    "        print(f\"ðŸƒ Running query {i+1}/10: {query[:50]}...\")\n",
    "        \n",
    "        query_results = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            response = client.query(query)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if response:\n",
    "                query_results.append({\n",
    "                    'run': run + 1,\n",
    "                    'response_time': end_time - start_time,\n",
    "                    'server_time': response.get('response_time_ms', 0) / 1000,\n",
    "                    'confidence': response.get('confidence_score', 0),\n",
    "                    'num_sources': len(response.get('retrieved_documents', [])),\n",
    "                    'answer_length': len(response.get('answer', '')),\n",
    "                    'success': True\n",
    "                })\n",
    "            else:\n",
    "                query_results.append({\n",
    "                    'run': run + 1,\n",
    "                    'response_time': end_time - start_time,\n",
    "                    'success': False\n",
    "                })\n",
    "            \n",
    "            time.sleep(0.1)  # Small delay between runs\n",
    "        \n",
    "        # Calculate statistics for this query\n",
    "        successful_runs = [r for r in query_results if r['success']]\n",
    "        \n",
    "        if successful_runs:\n",
    "            results.append({\n",
    "                'query_id': i,\n",
    "                'query': query,\n",
    "                'category': query_data['category'],\n",
    "                'difficulty': query_data['difficulty'],\n",
    "                'avg_response_time': np.mean([r['response_time'] for r in successful_runs]),\n",
    "                'std_response_time': np.std([r['response_time'] for r in successful_runs]),\n",
    "                'avg_server_time': np.mean([r['server_time'] for r in successful_runs]),\n",
    "                'avg_confidence': np.mean([r['confidence'] for r in successful_runs]),\n",
    "                'avg_sources': np.mean([r['num_sources'] for r in successful_runs]),\n",
    "                'avg_answer_length': np.mean([r['answer_length'] for r in successful_runs]),\n",
    "                'success_rate': len(successful_runs) / num_runs,\n",
    "                'runs': query_results\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance benchmark\n",
    "print(\"ðŸš€ Starting performance benchmark...\")\n",
    "performance_results = run_performance_benchmark(client, evaluation_queries)\n",
    "\n",
    "if performance_results:\n",
    "    perf_df = pd.DataFrame(performance_results)\n",
    "    print(f\"âœ… Completed performance benchmark with {len(performance_results)} queries\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\nðŸ“Š Performance Summary:\")\n",
    "    print(f\"Average Response Time: {perf_df['avg_response_time'].mean():.3f}s Â± {perf_df['avg_response_time'].std():.3f}s\")\n",
    "    print(f\"Average Server Time: {perf_df['avg_server_time'].mean():.3f}s Â± {perf_df['avg_server_time'].std():.3f}s\")\n",
    "    print(f\"Average Confidence: {perf_df['avg_confidence'].mean():.3f} Â± {perf_df['avg_confidence'].std():.3f}\")\n",
    "    print(f\"Average Sources Retrieved: {perf_df['avg_sources'].mean():.1f} Â± {perf_df['avg_sources'].std():.1f}\")\n",
    "    print(f\"Success Rate: {perf_df['success_rate'].mean():.3f} ({perf_df['success_rate'].mean()*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"âŒ Performance benchmark failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance results\n",
    "if 'perf_df' in locals() and not perf_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RAG System Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Response time distribution\n",
    "    axes[0, 0].hist(perf_df['avg_response_time'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Response Time Distribution')\n",
    "    axes[0, 0].set_xlabel('Response Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(perf_df['avg_response_time'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Confidence scores\n",
    "    axes[0, 1].scatter(range(len(perf_df)), perf_df['avg_confidence'], alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Confidence Scores by Query')\n",
    "    axes[0, 1].set_xlabel('Query Index')\n",
    "    axes[0, 1].set_ylabel('Confidence Score')\n",
    "    axes[0, 1].axhline(perf_df['avg_confidence'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Response time vs confidence\n",
    "    axes[1, 0].scatter(perf_df['avg_response_time'], perf_df['avg_confidence'], alpha=0.7, color='purple')\n",
    "    axes[1, 0].set_title('Response Time vs Confidence')\n",
    "    axes[1, 0].set_xlabel('Response Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('Confidence Score')\n",
    "    \n",
    "    # Performance by category\n",
    "    category_perf = perf_df.groupby('category')['avg_response_time'].mean()\n",
    "    axes[1, 1].bar(category_perf.index, category_perf.values, alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_title('Average Response Time by Category')\n",
    "    axes[1, 1].set_xlabel('Category')\n",
    "    axes[1, 1].set_ylabel('Response Time (seconds)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance statistics by difficulty\n",
    "    print(\"\\nðŸ“ˆ Performance by Difficulty Level:\")\n",
    "    difficulty_stats = perf_df.groupby('difficulty').agg({\n",
    "        'avg_response_time': ['mean', 'std'],\n",
    "        'avg_confidence': ['mean', 'std'],\n",
    "        'success_rate': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(difficulty_stats)\n",
    "else:\n",
    "    print(\"No performance data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Evaluation with RAGAS {#quality}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect responses for quality evaluation\n",
    "def collect_rag_responses(client, queries, max_queries=10):\n",
    "    \"\"\"Collect RAG responses for quality evaluation\"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for i, query_data in enumerate(queries[:max_queries]):\n",
    "        query = query_data['query']\n",
    "        print(f\"ðŸ“ Collecting response {i+1}/{max_queries}: {query[:50]}...\")\n",
    "        \n",
    "        response = client.query(query)\n",
    "        \n",
    "        if response and response.get('answer'):\n",
    "            contexts = [doc.get('content', '') for doc in response.get('retrieved_documents', [])]\n",
    "            \n",
    "            responses.append({\n",
    "                'question': query,\n",
    "                'answer': response['answer'],\n",
    "                'contexts': contexts,\n",
    "                'category': query_data['category'],\n",
    "                'difficulty': query_data['difficulty'],\n",
    "                'confidence': response.get('confidence_score', 0),\n",
    "                'response_time': response.get('response_time_ms', 0) / 1000,\n",
    "                'sources': [doc.get('filename', 'unknown') for doc in response.get('retrieved_documents', [])]\n",
    "            })\n",
    "        else:\n",
    "            print(f\"âŒ Failed to get response for query: {query[:50]}...\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Collect responses\n",
    "print(\"ðŸ”„ Collecting RAG responses for quality evaluation...\")\n",
    "rag_responses = collect_rag_responses(client, evaluation_queries, max_queries=8)\n",
    "\n",
    "if rag_responses:\n",
    "    print(f\"âœ… Collected {len(rag_responses)} responses for evaluation\")\n",
    "    \n",
    "    # Display sample response\n",
    "    print(\"\\nðŸ“‹ Sample Response:\")\n",
    "    sample = rag_responses[0]\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"Answer: {sample['answer'][:200]}...\")\n",
    "    print(f\"Sources: {sample['sources'][:3]}\")\n",
    "    print(f\"Confidence: {sample['confidence']:.3f}\")\n",
    "else:\n",
    "    print(\"âŒ No responses collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom quality evaluation metrics\n",
    "def evaluate_answer_quality(responses):\n",
    "    \"\"\"Evaluate answer quality using various metrics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for response in responses:\n",
    "        question = response['question']\n",
    "        answer = response['answer']\n",
    "        contexts = response['contexts']\n",
    "        \n",
    "        # Length metrics\n",
    "        answer_length = len(answer.split())\n",
    "        context_length = sum(len(ctx.split()) for ctx in contexts)\n",
    "        \n",
    "        # Source diversity\n",
    "        unique_sources = len(set(response['sources']))\n",
    "        source_diversity = unique_sources / len(response['sources']) if response['sources'] else 0\n",
    "        \n",
    "        # Basic completeness check (answer has substantive content)\n",
    "        completeness = 1.0 if answer_length > 20 else answer_length / 20.0\n",
    "        \n",
    "        # Citation analysis (simple check for source references)\n",
    "        has_citations = any(source.replace('.pdf', '').replace('.md', '').lower() in answer.lower() \n",
    "                          for source in response['sources'])\n",
    "        \n",
    "        # Question-answer relevance (simple keyword overlap)\n",
    "        question_words = set(question.lower().split())\n",
    "        answer_words = set(answer.lower().split())\n",
    "        relevance_score = len(question_words.intersection(answer_words)) / len(question_words)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'category': response['category'],\n",
    "            'difficulty': response['difficulty'],\n",
    "            'answer_length': answer_length,\n",
    "            'context_length': context_length,\n",
    "            'unique_sources': unique_sources,\n",
    "            'source_diversity': source_diversity,\n",
    "            'completeness': completeness,\n",
    "            'has_citations': has_citations,\n",
    "            'relevance_score': relevance_score,\n",
    "            'confidence': response['confidence'],\n",
    "            'response_time': response['response_time']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate quality\n",
    "if rag_responses:\n",
    "    quality_results = evaluate_answer_quality(rag_responses)\n",
    "    quality_df = pd.DataFrame(quality_results)\n",
    "    \n",
    "    print(\"ðŸ“Š Quality Evaluation Results:\")\n",
    "    print(f\"Average Answer Length: {quality_df['answer_length'].mean():.1f} words\")\n",
    "    print(f\"Average Source Diversity: {quality_df['source_diversity'].mean():.3f}\")\n",
    "    print(f\"Average Completeness: {quality_df['completeness'].mean():.3f}\")\n",
    "    print(f\"Citation Rate: {quality_df['has_citations'].mean():.3f} ({quality_df['has_citations'].mean()*100:.1f}%)\")\n",
    "    print(f\"Average Relevance: {quality_df['relevance_score'].mean():.3f}\")\n",
    "    \n",
    "    # Quality by category\n",
    "    print(\"\\nðŸ“ˆ Quality by Category:\")\n",
    "    category_quality = quality_df.groupby('category').agg({\n",
    "        'completeness': 'mean',\n",
    "        'relevance_score': 'mean',\n",
    "        'source_diversity': 'mean',\n",
    "        'has_citations': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(category_quality)\n",
    "else:\n",
    "    print(\"âŒ No responses available for quality evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quality metrics\n",
    "if 'quality_df' in locals() and not quality_df.empty:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('RAG System Quality Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Answer length distribution\n",
    "    axes[0, 0].hist(quality_df['answer_length'], bins=10, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Answer Length Distribution')\n",
    "    axes[0, 0].set_xlabel('Answer Length (words)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Confidence vs Relevance\n",
    "    axes[0, 1].scatter(quality_df['confidence'], quality_df['relevance_score'], alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Confidence vs Relevance')\n",
    "    axes[0, 1].set_xlabel('Confidence Score')\n",
    "    axes[0, 1].set_ylabel('Relevance Score')\n",
    "    \n",
    "    # Source diversity\n",
    "    axes[0, 2].bar(range(len(quality_df)), quality_df['source_diversity'], alpha=0.7, color='orange')\n",
    "    axes[0, 2].set_title('Source Diversity by Query')\n",
    "    axes[0, 2].set_xlabel('Query Index')\n",
    "    axes[0, 2].set_ylabel('Source Diversity')\n",
    "    \n",
    "    # Quality metrics by category\n",
    "    category_means = quality_df.groupby('category')[['completeness', 'relevance_score', 'source_diversity']].mean()\n",
    "    category_means.plot(kind='bar', ax=axes[1, 0], alpha=0.7)\n",
    "    axes[1, 0].set_title('Quality Metrics by Category')\n",
    "    axes[1, 0].set_xlabel('Category')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Response time vs quality\n",
    "    axes[1, 1].scatter(quality_df['response_time'], quality_df['completeness'], alpha=0.7, color='purple')\n",
    "    axes[1, 1].set_title('Response Time vs Completeness')\n",
    "    axes[1, 1].set_xlabel('Response Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Completeness Score')\n",
    "    \n",
    "    # Citation rate by difficulty\n",
    "    difficulty_citations = quality_df.groupby('difficulty')['has_citations'].mean()\n",
    "    axes[1, 2].bar(difficulty_citations.index, difficulty_citations.values, alpha=0.7, color='red')\n",
    "    axes[1, 2].set_title('Citation Rate by Difficulty')\n",
    "    axes[1, 2].set_xlabel('Difficulty Level')\n",
    "    axes[1, 2].set_ylabel('Citation Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No quality data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieval Analysis {#retrieval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze retrieval performance\n",
    "def analyze_retrieval_performance(responses):\n",
    "    \"\"\"Analyze retrieval component performance\"\"\"\n",
    "    retrieval_stats = []\n",
    "    \n",
    "    for response in responses:\n",
    "        question = response['question']\n",
    "        docs = response.get('retrieved_documents', [])\n",
    "        \n",
    "        if not docs:\n",
    "            continue\n",
    "            \n",
    "        # Retrieval metrics\n",
    "        num_docs = len(docs)\n",
    "        avg_relevance = np.mean([doc.get('relevance_score', 0) for doc in docs])\n",
    "        max_relevance = max([doc.get('relevance_score', 0) for doc in docs])\n",
    "        min_relevance = min([doc.get('relevance_score', 0) for doc in docs])\n",
    "        relevance_std = np.std([doc.get('relevance_score', 0) for doc in docs])\n",
    "        \n",
    "        # Source analysis\n",
    "        sources = [doc.get('filename', 'unknown') for doc in docs]\n",
    "        unique_sources = len(set(sources))\n",
    "        source_distribution = {src: sources.count(src) for src in set(sources)}\n",
    "        \n",
    "        # Content analysis\n",
    "        total_content_length = sum(len(doc.get('content', '')) for doc in docs)\n",
    "        avg_content_length = total_content_length / num_docs if num_docs > 0 else 0\n",
    "        \n",
    "        retrieval_stats.append({\n",
    "            'question': question,\n",
    "            'category': response['category'],\n",
    "            'difficulty': response['difficulty'],\n",
    "            'num_docs': num_docs,\n",
    "            'avg_relevance': avg_relevance,\n",
    "            'max_relevance': max_relevance,\n",
    "            'min_relevance': min_relevance,\n",
    "            'relevance_std': relevance_std,\n",
    "            'unique_sources': unique_sources,\n",
    "            'source_diversity': unique_sources / num_docs if num_docs > 0 else 0,\n",
    "            'total_content_length': total_content_length,\n",
    "            'avg_content_length': avg_content_length,\n",
    "            'source_distribution': source_distribution\n",
    "        })\n",
    "    \n",
    "    return retrieval_stats\n",
    "\n",
    "# Analyze retrieval\n",
    "if rag_responses:\n",
    "    # First, let's get detailed responses with retrieved documents\n",
    "    detailed_responses = []\n",
    "    for response in rag_responses[:5]:  # Analyze first 5 responses in detail\n",
    "        query_response = client.query(response['question'])\n",
    "        if query_response:\n",
    "            detailed_response = response.copy()\n",
    "            detailed_response['retrieved_documents'] = query_response.get('retrieved_documents', [])\n",
    "            detailed_responses.append(detailed_response)\n",
    "    \n",
    "    retrieval_analysis = analyze_retrieval_performance(detailed_responses)\n",
    "    \n",
    "    if retrieval_analysis:\n",
    "        retrieval_df = pd.DataFrame(retrieval_analysis)\n",
    "        \n",
    "        print(\"ðŸ“Š Retrieval Analysis Results:\")\n",
    "        print(f\"Average Documents Retrieved: {retrieval_df['num_docs'].mean():.1f} Â± {retrieval_df['num_docs'].std():.1f}\")\n",
    "        print(f\"Average Relevance Score: {retrieval_df['avg_relevance'].mean():.3f} Â± {retrieval_df['avg_relevance'].std():.3f}\")\n",
    "        print(f\"Average Source Diversity: {retrieval_df['source_diversity'].mean():.3f} Â± {retrieval_df['source_diversity'].std():.3f}\")\n",
    "        print(f\"Average Content Length: {retrieval_df['avg_content_length'].mean():.0f} characters\")\n",
    "        \n",
    "        # Most common sources\n",
    "        all_sources = {}\n",
    "        for dist in retrieval_df['source_distribution']:\n",
    "            for source, count in dist.items():\n",
    "                all_sources[source] = all_sources.get(source, 0) + count\n",
    "        \n",
    "        print(\"\\nðŸ“š Most Retrieved Sources:\")\n",
    "        top_sources = sorted(all_sources.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        for source, count in top_sources:\n",
    "            print(f\"  {source}: {count} times\")\n",
    "    else:\n",
    "        print(\"âŒ No retrieval data available for analysis\")\n",
    "else:\n",
    "    print(\"âŒ No responses available for retrieval analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation Assessment {#generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze generation quality\n",
    "def analyze_generation_quality(responses):\n",
    "    \"\"\"Analyze answer generation quality\"\"\"\n",
    "    generation_stats = []\n",
    "    \n",
    "    for response in responses:\n",
    "        answer = response['answer']\n",
    "        question = response['question']\n",
    "        \n",
    "        # Text statistics\n",
    "        word_count = len(answer.split())\n",
    "        sentence_count = len([s for s in answer.split('.') if s.strip()])\n",
    "        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "        \n",
    "        # Vocabulary richness\n",
    "        unique_words = len(set(answer.lower().split()))\n",
    "        vocabulary_richness = unique_words / word_count if word_count > 0 else 0\n",
    "        \n",
    "        # Structure analysis\n",
    "        has_bullets = 'â€¢' in answer or '-' in answer\n",
    "        has_numbers = any(char.isdigit() for char in answer)\n",
    "        has_sections = any(header in answer.lower() for header in ['introduction', 'conclusion', 'overview', 'summary'])\n",
    "        \n",
    "        # Question alignment\n",
    "        question_words = set(question.lower().split())\n",
    "        answer_words = set(answer.lower().split())\n",
    "        topic_coverage = len(question_words.intersection(answer_words)) / len(question_words)\n",
    "        \n",
    "        generation_stats.append({\n",
    "            'question': question,\n",
    "            'category': response['category'],\n",
    "            'difficulty': response['difficulty'],\n",
    "            'word_count': word_count,\n",
    "            'sentence_count': sentence_count,\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'unique_words': unique_words,\n",
    "            'vocabulary_richness': vocabulary_richness,\n",
    "            'has_bullets': has_bullets,\n",
    "            'has_numbers': has_numbers,\n",
    "            'has_sections': has_sections,\n",
    "            'topic_coverage': topic_coverage,\n",
    "            'confidence': response['confidence']\n",
    "        })\n",
    "    \n",
    "    return generation_stats\n",
    "\n",
    "# Analyze generation\n",
    "if rag_responses:\n",
    "    generation_analysis = analyze_generation_quality(rag_responses)\n",
    "    generation_df = pd.DataFrame(generation_analysis)\n",
    "    \n",
    "    print(\"ðŸ“ Generation Analysis Results:\")\n",
    "    print(f\"Average Word Count: {generation_df['word_count'].mean():.1f} Â± {generation_df['word_count'].std():.1f}\")\n",
    "    print(f\"Average Sentence Count: {generation_df['sentence_count'].mean():.1f} Â± {generation_df['sentence_count'].std():.1f}\")\n",
    "    print(f\"Average Sentence Length: {generation_df['avg_sentence_length'].mean():.1f} words\")\n",
    "    print(f\"Average Vocabulary Richness: {generation_df['vocabulary_richness'].mean():.3f}\")\n",
    "    print(f\"Average Topic Coverage: {generation_df['topic_coverage'].mean():.3f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Structure Features:\")\n",
    "    print(f\"Answers with Bullet Points: {generation_df['has_bullets'].mean():.1%}\")\n",
    "    print(f\"Answers with Numbers: {generation_df['has_numbers'].mean():.1%}\")\n",
    "    print(f\"Answers with Sections: {generation_df['has_sections'].mean():.1%}\")\n",
    "    \n",
    "    # Generation quality by difficulty\n",
    "    print(\"\\nðŸ“ˆ Generation Quality by Difficulty:\")\n",
    "    difficulty_gen = generation_df.groupby('difficulty').agg({\n",
    "        'word_count': 'mean',\n",
    "        'vocabulary_richness': 'mean',\n",
    "        'topic_coverage': 'mean',\n",
    "        'confidence': 'mean'\n",
    "    }).round(3)\n",
    "    print(difficulty_gen)\n",
    "else:\n",
    "    print(\"âŒ No responses available for generation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparative Analysis {#comparison}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different query types\n",
    "def compare_query_types(client, test_query=\"What is machine learning?\"):\n",
    "    \"\"\"Compare different query types on the same question\"\"\"\n",
    "    query_types = ['semantic', 'keyword', 'hybrid']\n",
    "    results = []\n",
    "    \n",
    "    for query_type in query_types:\n",
    "        print(f\"Testing {query_type} search...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = client.query(test_query, query_type=query_type, top_k=5)\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        if response:\n",
    "            results.append({\n",
    "                'query_type': query_type,\n",
    "                'response_time': response_time,\n",
    "                'answer_length': len(response.get('answer', '')),\n",
    "                'confidence': response.get('confidence_score', 0),\n",
    "                'num_sources': len(response.get('retrieved_documents', [])),\n",
    "                'avg_relevance': np.mean([doc.get('relevance_score', 0) \n",
    "                                        for doc in response.get('retrieved_documents', [])]) if response.get('retrieved_documents') else 0\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "print(\"ðŸ”„ Comparing query types...\")\n",
    "comparison_results = compare_query_types(client)\n",
    "\n",
    "if comparison_results:\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Query Type Comparison:\")\n",
    "    print(comparison_df.round(3))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle('Query Type Comparison', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Response time\n",
    "    axes[0].bar(comparison_df['query_type'], comparison_df['response_time'], alpha=0.7)\n",
    "    axes[0].set_title('Response Time')\n",
    "    axes[0].set_ylabel('Time (seconds)')\n",
    "    \n",
    "    # Confidence\n",
    "    axes[1].bar(comparison_df['query_type'], comparison_df['confidence'], alpha=0.7, color='green')\n",
    "    axes[1].set_title('Confidence Score')\n",
    "    axes[1].set_ylabel('Confidence')\n",
    "    \n",
    "    # Average relevance\n",
    "    axes[2].bar(comparison_df['query_type'], comparison_df['avg_relevance'], alpha=0.7, color='orange')\n",
    "    axes[2].set_title('Average Relevance')\n",
    "    axes[2].set_ylabel('Relevance Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best performing query type\n",
    "    best_overall = comparison_df.loc[comparison_df['confidence'].idxmax()]\n",
    "    print(f\"\\nðŸ† Best overall performance: {best_overall['query_type']} (confidence: {best_overall['confidence']:.3f})\")\n",
    "else:\n",
    "    print(\"âŒ Query type comparison failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimization Recommendations {#optimization}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimization recommendations\n",
    "def generate_recommendations(performance_df=None, quality_df=None, generation_df=None, comparison_df=None):\n",
    "    \"\"\"Generate optimization recommendations based on evaluation results\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Performance recommendations\n",
    "    if performance_df is not None and not performance_df.empty:\n",
    "        avg_response_time = performance_df['avg_response_time'].mean()\n",
    "        \n",
    "        if avg_response_time > 3.0:\n",
    "            recommendations.append({\n",
    "                'category': 'Performance',\n",
    "                'priority': 'High',\n",
    "                'issue': f'Average response time is {avg_response_time:.2f}s, exceeding 3s threshold',\n",
    "                'recommendation': 'Consider caching frequently asked questions, optimizing vector search parameters, or scaling infrastructure'\n",
    "            })\n",
    "        \n",
    "        avg_confidence = performance_df['avg_confidence'].mean()\n",
    "        if avg_confidence < 0.8:\n",
    "            recommendations.append({\n",
    "                'category': 'Quality',\n",
    "                'priority': 'Medium',\n",
    "                'issue': f'Average confidence score is {avg_confidence:.3f}, below 0.8 threshold',\n",
    "                'recommendation': 'Review retrieval parameters, consider retraining embeddings, or improving document quality'\n",
    "            })\n",
    "    \n",
    "    # Quality recommendations\n",
    "    if quality_df is not None and not quality_df.empty:\n",
    "        citation_rate = quality_df['has_citations'].mean()\n",
    "        if citation_rate < 0.7:\n",
    "            recommendations.append({\n",
    "                'category': 'Quality',\n",
    "                'priority': 'Medium', \n",
    "                'issue': f'Citation rate is {citation_rate:.1%}, below 70% target',\n",
    "                'recommendation': 'Improve prompt engineering to encourage source attribution in responses'\n",
    "            })\n",
    "        \n",
    "        avg_relevance = quality_df['relevance_score'].mean()\n",
    "        if avg_relevance < 0.6:\n",
    "            recommendations.append({\n",
    "                'category': 'Retrieval',\n",
    "                'priority': 'High',\n",
    "                'issue': f'Average relevance score is {avg_relevance:.3f}, indicating poor retrieval quality',\n",
    "                'recommendation': 'Optimize embedding model selection, improve document chunking strategy, or tune retrieval parameters'\n",
    "            })\n",
    "    \n",
    "    # Generation recommendations\n",
    "    if generation_df is not None and not generation_df.empty:\n",
    "        avg_word_count = generation_df['word_count'].mean()\n",
    "        if avg_word_count < 50:\n",
    "            recommendations.append({\n",
    "                'category': 'Generation',\n",
    "                'priority': 'Low',\n",
    "                'issue': f'Average answer length is {avg_word_count:.0f} words, which may be too brief',\n",
    "                'recommendation': 'Adjust prompt instructions to encourage more comprehensive responses'\n",
    "            })\n",
    "        \n",
    "        vocab_richness = generation_df['vocabulary_richness'].mean()\n",
    "        if vocab_richness < 0.4:\n",
    "            recommendations.append({\n",
    "                'category': 'Generation',\n",
    "                'priority': 'Low',\n",
    "                'issue': f'Vocabulary richness is {vocab_richness:.3f}, indicating repetitive language',\n",
    "                'recommendation': 'Consider using temperature settings or different generation parameters for more diverse responses'\n",
    "            })\n",
    "    \n",
    "    # Query type recommendations\n",
    "    if comparison_df is not None and not comparison_df.empty:\n",
    "        best_query_type = comparison_df.loc[comparison_df['confidence'].idxmax(), 'query_type']\n",
    "        if best_query_type != 'hybrid':\n",
    "            recommendations.append({\n",
    "                'category': 'Configuration',\n",
    "                'priority': 'Medium',\n",
    "                'issue': f'{best_query_type} search performs better than hybrid for test queries',\n",
    "                'recommendation': f'Consider making {best_query_type} search the default, or tune hybrid search parameters'\n",
    "            })\n",
    "    \n",
    "    # General recommendations\n",
    "    recommendations.extend([\n",
    "        {\n",
    "            'category': 'Monitoring',\n",
    "            'priority': 'Medium',\n",
    "            'issue': 'Need for continuous evaluation',\n",
    "            'recommendation': 'Set up automated evaluation pipelines to monitor system performance over time'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Data Quality',\n",
    "            'priority': 'Medium',\n",
    "            'issue': 'Document quality affects retrieval performance',\n",
    "            'recommendation': 'Regularly audit and update knowledge base content for accuracy and relevance'\n",
    "        },\n",
    "        {\n",
    "            'category': 'User Experience',\n",
    "            'priority': 'Low',\n",
    "            'issue': 'User feedback integration',\n",
    "            'recommendation': 'Implement user feedback collection to identify problem areas and improvement opportunities'\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(\n",
    "    performance_df=perf_df if 'perf_df' in locals() else None,\n",
    "    quality_df=quality_df if 'quality_df' in locals() else None,\n",
    "    generation_df=generation_df if 'generation_df' in locals() else None,\n",
    "    comparison_df=comparison_df if 'comparison_df' in locals() else None\n",
    ")\n",
    "\n",
    "print(\"ðŸ’¡ Optimization Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    priority_emoji = {\"High\": \"ðŸ”´\", \"Medium\": \"ðŸŸ¡\", \"Low\": \"ðŸŸ¢\"}[rec['priority']]\n",
    "    print(f\"\\n{i}. [{rec['category']}] {priority_emoji} {rec['priority']} Priority\")\n",
    "    print(f\"   Issue: {rec['issue']}\")\n",
    "    print(f\"   Recommendation: {rec['recommendation']}\")\n",
    "\n",
    "# Priority summary\n",
    "priority_counts = {}\n",
    "for rec in recommendations:\n",
    "    priority = rec['priority']\n",
    "    priority_counts[priority] = priority_counts.get(priority, 0) + 1\n",
    "\n",
    "print(f\"\\nðŸ“Š Recommendation Summary:\")\n",
    "for priority, count in sorted(priority_counts.items(), key=lambda x: ['High', 'Medium', 'Low'].index(x[0])):\n",
    "    emoji = {\"High\": \"ðŸ”´\", \"Medium\": \"ðŸŸ¡\", \"Low\": \"ðŸŸ¢\"}[priority]\n",
    "    print(f\"   {emoji} {priority}: {count} recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reporting and Visualization {#reporting}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "def generate_evaluation_report():\n",
    "    \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "    report = {\n",
    "        'evaluation_timestamp': datetime.now().isoformat(),\n",
    "        'system_info': {\n",
    "            'base_url': config['base_url'],\n",
    "            'evaluation_queries': len(evaluation_queries),\n",
    "            'random_seed': config['random_seed']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Performance metrics\n",
    "    if 'perf_df' in locals() and not perf_df.empty:\n",
    "        report['performance'] = {\n",
    "            'avg_response_time': float(perf_df['avg_response_time'].mean()),\n",
    "            'std_response_time': float(perf_df['avg_response_time'].std()),\n",
    "            'avg_confidence': float(perf_df['avg_confidence'].mean()),\n",
    "            'success_rate': float(perf_df['success_rate'].mean()),\n",
    "            'queries_evaluated': len(perf_df)\n",
    "        }\n",
    "    \n",
    "    # Quality metrics\n",
    "    if 'quality_df' in locals() and not quality_df.empty:\n",
    "        report['quality'] = {\n",
    "            'avg_answer_length': float(quality_df['answer_length'].mean()),\n",
    "            'source_diversity': float(quality_df['source_diversity'].mean()),\n",
    "            'completeness': float(quality_df['completeness'].mean()),\n",
    "            'citation_rate': float(quality_df['has_citations'].mean()),\n",
    "            'relevance_score': float(quality_df['relevance_score'].mean())\n",
    "        }\n",
    "    \n",
    "    # Generation metrics\n",
    "    if 'generation_df' in locals() and not generation_df.empty:\n",
    "        report['generation'] = {\n",
    "            'avg_word_count': float(generation_df['word_count'].mean()),\n",
    "            'vocabulary_richness': float(generation_df['vocabulary_richness'].mean()),\n",
    "            'topic_coverage': float(generation_df['topic_coverage'].mean()),\n",
    "            'structure_features': {\n",
    "                'has_bullets_rate': float(generation_df['has_bullets'].mean()),\n",
    "                'has_numbers_rate': float(generation_df['has_numbers'].mean()),\n",
    "                'has_sections_rate': float(generation_df['has_sections'].mean())\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Query type comparison\n",
    "    if 'comparison_df' in locals() and not comparison_df.empty:\n",
    "        report['query_types'] = comparison_df.to_dict('records')\n",
    "    \n",
    "    # Recommendations\n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save report\n",
    "evaluation_report = generate_evaluation_report()\n",
    "\n",
    "# Save report as JSON\n",
    "import json\n",
    "report_filename = f\"rag_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(report_filename, 'w') as f:\n",
    "    json.dump(evaluation_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"ðŸ“Š Evaluation report saved as: {report_filename}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ RAG KNOWLEDGE ASSISTANT - EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'performance' in evaluation_report:\n",
    "    perf = evaluation_report['performance']\n",
    "    print(f\"\\nâš¡ PERFORMANCE:\")\n",
    "    print(f\"  Average Response Time: {perf['avg_response_time']:.3f}s Â± {perf['std_response_time']:.3f}s\")\n",
    "    print(f\"  Success Rate: {perf['success_rate']:.1%}\")\n",
    "    print(f\"  Average Confidence: {perf['avg_confidence']:.3f}\")\n",
    "\n",
    "if 'quality' in evaluation_report:\n",
    "    qual = evaluation_report['quality']\n",
    "    print(f\"\\nðŸ“‹ QUALITY:\")\n",
    "    print(f\"  Citation Rate: {qual['citation_rate']:.1%}\")\n",
    "    print(f\"  Completeness: {qual['completeness']:.3f}\")\n",
    "    print(f\"  Relevance Score: {qual['relevance_score']:.3f}\")\n",
    "    print(f\"  Source Diversity: {qual['source_diversity']:.3f}\")\n",
    "\n",
    "if 'generation' in evaluation_report:\n",
    "    gen = evaluation_report['generation']\n",
    "    print(f\"\\nâœï¸ GENERATION:\")\n",
    "    print(f\"  Average Word Count: {gen['avg_word_count']:.0f} words\")\n",
    "    print(f\"  Vocabulary Richness: {gen['vocabulary_richness']:.3f}\")\n",
    "    print(f\"  Topic Coverage: {gen['topic_coverage']:.3f}\")\n",
    "\n",
    "high_priority_recs = [r for r in recommendations if r['priority'] == 'High']\n",
    "print(f\"\\nðŸ”´ HIGH PRIORITY RECOMMENDATIONS: {len(high_priority_recs)}\")\n",
    "for rec in high_priority_recs:\n",
    "    print(f\"  â€¢ [{rec['category']}] {rec['issue']}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Full evaluation report: {report_filename}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive evaluation notebook provides:\n",
    "\n",
    "1. **Performance Analysis**: Response times, throughput, and system reliability\n",
    "2. **Quality Assessment**: Answer quality, citation accuracy, and relevance scoring\n",
    "3. **Retrieval Evaluation**: Document retrieval effectiveness and source diversity\n",
    "4. **Generation Analysis**: Answer completeness, structure, and vocabulary richness\n",
    "5. **Comparative Studies**: Different query types and configuration comparisons\n",
    "6. **Optimization Recommendations**: Actionable insights for system improvement\n",
    "\n",
    "### Key Metrics to Monitor:\n",
    "- **Response Time**: < 3 seconds for good user experience\n",
    "- **Confidence Score**: > 0.8 for reliable responses\n",
    "- **Citation Rate**: > 70% for transparency\n",
    "- **Success Rate**: > 95% for reliability\n",
    "- **Source Diversity**: Varied sources for comprehensive answers\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement high-priority recommendations\n",
    "2. Set up automated evaluation pipelines\n",
    "3. Monitor system performance continuously\n",
    "4. Collect user feedback for improvement\n",
    "5. Regular model and data quality assessments\n",
    "\n",
    "This evaluation framework should be run regularly to ensure optimal RAG system performance and identify areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python", 
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
